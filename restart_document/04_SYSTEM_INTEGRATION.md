# ğŸŒ‰ System Integration & Data Bridge

## ğŸ¯ **Integration Overview**

The TEP Simulator and FaultExplainer projects are connected through a sophisticated data bridge that enables real-time fault analysis of live simulation data.

## ğŸ”„ **Integration Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TEP Simulator     â”‚    â”‚   Data Bridge        â”‚    â”‚  FaultExplainer     â”‚
â”‚   (Project 1)       â”‚â”€â”€â”€â–¶â”‚                      â”‚â”€â”€â”€â–¶â”‚   (Project 2)       â”‚
â”‚                     â”‚    â”‚ tep_faultexplainer   â”‚    â”‚                     â”‚
â”‚ â€¢ Real tep2py       â”‚    â”‚ _bridge.py           â”‚    â”‚ â€¢ PCA Analysis      â”‚
â”‚ â€¢ 52 variables      â”‚    â”‚                      â”‚    â”‚ â€¢ Multi-LLM         â”‚
â”‚ â€¢ CSV export        â”‚    â”‚ â€¢ Variable mapping   â”‚    â”‚ â€¢ Web interface     â”‚
â”‚ â€¢ Fault injection   â”‚    â”‚ â€¢ Time windowing     â”‚    â”‚ â€¢ Fault diagnosis   â”‚
â”‚                     â”‚    â”‚ â€¢ Format conversion  â”‚    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸŒ‰ **Data Bridge Component**

### **Core Bridge Class**
**File**: `tep_faultexplainer_bridge.py`

```python
class TEPFaultExplainerBridge:
    def __init__(self):
        self.live_data_file = "data/live_tep_data.csv"
        self.faultexplainer_url = "http://localhost:8000"
        self.window_size = 20  # PCA analysis window
        self.data_buffer = deque(maxlen=self.window_size)
        self.last_processed_step = -1
```

### **Key Responsibilities**

#### **1. Variable Mapping**
```python
# TEP to FaultExplainer variable mapping
VARIABLE_MAPPING = {
    'XMEAS_1': 'A Feed',
    'XMEAS_2': 'D Feed', 
    'XMEAS_7': 'Reactor Pressure',
    'XMEAS_8': 'Reactor Level',
    'XMEAS_9': 'Reactor Temperature',
    'XMEAS_11': 'Product Sep Temp',
    'XMEAS_12': 'Product Sep Level',
    # ... 22 key variables mapped
}
```

#### **2. Time Window Management**
```python
def process_data_point(self, tep_data):
    # Map TEP variables to FaultExplainer format
    mapped_data = self.map_tep_to_faultexplainer(tep_data)
    
    # Add to sliding window buffer
    self.data_buffer.append(mapped_data)
    
    # When buffer is full, send to FaultExplainer
    if len(self.data_buffer) == self.window_size:
        window_data = list(self.data_buffer)
        result = self.send_to_faultexplainer(window_data)
        return result
```

#### **3. API Communication**
```python
def send_to_faultexplainer(self, window_data):
    payload = {
        "data": window_data,
        "window_size": len(window_data),
        "source": "live_tep_simulation",
        "timestamp": datetime.now().isoformat()
    }
    
    response = requests.post(
        f"{self.faultexplainer_url}/analyze",
        json=payload,
        timeout=10
    )
    
    return response.json()
```

## ğŸ“Š **Data Flow Pipeline**

### **Step 1: TEP Data Generation**
```python
# TEP Simulator generates data
tep_data = {
    'timestamp': time.time(),
    'step': current_step,
    'fault_type': active_fault,
    'fault_intensity': fault_level,
    'XMEAS_1': reactor_feed_a,
    'XMEAS_2': reactor_feed_d,
    # ... all 52 variables
}

# Save to CSV
df.to_csv('data/live_tep_data.csv', index=False)
```

### **Step 2: Bridge Processing**
```python
# Bridge monitors CSV file
new_data_points = bridge.read_new_data()

for data_point in new_data_points:
    # Convert TEP format to FaultExplainer format
    mapped_data = bridge.map_tep_to_faultexplainer(data_point)
    
    # Add to sliding window
    bridge.data_buffer.append(mapped_data)
    
    # Check if window is full
    if len(bridge.data_buffer) == 20:
        # Send to FaultExplainer for analysis
        result = bridge.send_to_faultexplainer(window_data)
```

### **Step 3: FaultExplainer Analysis**
```python
# FaultExplainer receives data
@app.post("/analyze")
async def analyze_fault(request: AnalysisRequest):
    # Extract window data
    window_data = request.data
    
    # Run PCA analysis
    anomalies = []
    for data_point in window_data:
        anomaly = pca_model.process_data_point(data_point)
        anomalies.append(anomaly)
    
    # If anomalies detected, trigger LLM analysis
    if any(anomalies):
        llm_result = await multi_llm_client.get_analysis_from_all_models(
            system_message=SYSTEM_MESSAGE,
            user_prompt=generate_fault_prompt(window_data)
        )
        return llm_result
```

## âš™ï¸ **Configuration & Timing**

### **Industrial Timing Hierarchy**
```python
# TEP Simulation timing
TEP_STEP_INTERVAL = 180  # seconds (3 minutes simulation time)

# PCA Analysis timing  
PCA_WINDOW_SIZE = 20     # data points
PCA_TRIGGER_FREQUENCY = 360  # seconds (6 minutes)

# LLM Analysis timing
LLM_TRIGGER_FREQUENCY = 720  # seconds (12 minutes)
```

### **Data Buffer Management**
```python
# Sliding window configuration
WINDOW_SIZE = 20         # PCA analysis window
BUFFER_OVERLAP = 10      # Keep last 10 points for continuity
MAX_BUFFER_SIZE = 100    # Prevent memory overflow

# Quality control
MIN_DATA_QUALITY = 0.95  # Reject incomplete data points
ANOMALY_THRESHOLD = 0.01 # TÂ² statistic threshold
```

## ğŸ”§ **Integration Points**

### **1. File-Based Integration**
```python
# TEP Simulator writes to CSV
TEP_OUTPUT_FILE = "data/live_tep_data.csv"

# Bridge monitors file changes
def monitor_csv_file():
    while True:
        if file_modified(TEP_OUTPUT_FILE):
            new_data = read_new_rows(TEP_OUTPUT_FILE)
            process_new_data(new_data)
        time.sleep(5)  # Check every 5 seconds
```

### **2. API Integration**
```python
# Bridge sends HTTP requests to FaultExplainer
FAULTEXPLAINER_ENDPOINTS = {
    'analyze': 'http://localhost:8000/analyze',
    'health': 'http://localhost:8000/health',
    'config': 'http://localhost:8000/config'
}

# Health check before sending data
def check_faultexplainer_status():
    try:
        response = requests.get(FAULTEXPLAINER_ENDPOINTS['health'])
        return response.status_code == 200
    except:
        return False
```

### **3. Real-time Monitoring**
```python
# System status tracking
class SystemMonitor:
    def __init__(self):
        self.tep_status = "unknown"
        self.bridge_status = "unknown" 
        self.faultexplainer_status = "unknown"
        self.last_analysis_time = None
        self.total_analyses = 0
    
    def update_status(self):
        self.tep_status = self.check_tep_simulator()
        self.bridge_status = self.check_bridge_process()
        self.faultexplainer_status = self.check_faultexplainer_api()
```

## ğŸš€ **Startup Sequence**

### **Complete System Startup**
```bash
# Terminal 1: TEP Simulator
source tep_env/bin/activate
python real_tep_simulator.py

# Terminal 2: FaultExplainer Backend  
cd external_repos/FaultExplainer-main/backend
source ../../../tep_env/bin/activate
python app.py

# Terminal 3: FaultExplainer Frontend
cd external_repos/FaultExplainer-main/frontend
npm run dev

# Terminal 4: Data Bridge
source tep_env/bin/activate
python tep_faultexplainer_bridge.py

# Terminal 5: System Monitor (optional)
source tep_env/bin/activate
python working_system_dashboard.py
```

### **Simplified Startup**
```bash
# Unified launcher (in development)
source tep_env/bin/activate
python run_complete_system.py
```

## ğŸ“ˆ **Performance Metrics**

### **Data Throughput**
- **TEP Generation**: 1 data point per 3 minutes (simulation time)
- **Bridge Processing**: ~20 data points per analysis window
- **PCA Analysis**: <1 second per window
- **LLM Analysis**: 5-30 seconds depending on model

### **System Resources**
- **Memory Usage**: ~500MB total (all components)
- **CPU Usage**: <10% during normal operation
- **Network**: Minimal (local HTTP requests only)
- **Storage**: ~1MB per hour of simulation data

## ğŸ” **Troubleshooting Integration**

### **Common Issues**
```python
# 1. Bridge can't find TEP data
if not os.path.exists("data/live_tep_data.csv"):
    print("âŒ TEP simulator not generating data")
    print("ğŸ’¡ Start TEP simulator first")

# 2. FaultExplainer not responding
if not check_faultexplainer_status():
    print("âŒ FaultExplainer backend not running")
    print("ğŸ’¡ Start backend: cd backend && python app.py")

# 3. Variable mapping errors
if len(mapped_data) != expected_variables:
    print("âŒ Variable mapping incomplete")
    print("ğŸ’¡ Check VARIABLE_MAPPING dictionary")
```

This integration provides **seamless real-time connection** between authentic TEP simulation and AI-powered fault analysis.
